{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-02-17T16:35:06.022906Z",
     "iopub.status.busy": "2023-02-17T16:35:06.022470Z",
     "iopub.status.idle": "2023-02-17T16:35:06.028854Z",
     "shell.execute_reply": "2023-02-17T16:35:06.027595Z",
     "shell.execute_reply.started": "2023-02-17T16:35:06.022871Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "# import numba\n",
    "# numba.set_num_threads(12) # 12 = cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# pd.set_option('use_numba', True)\n",
    "import polars as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T16:35:06.068388Z",
     "iopub.status.busy": "2023-02-17T16:35:06.067287Z",
     "iopub.status.idle": "2023-02-17T16:35:06.073857Z",
     "shell.execute_reply": "2023-02-17T16:35:06.072763Z",
     "shell.execute_reply.started": "2023-02-17T16:35:06.068320Z"
    }
   },
   "outputs": [],
   "source": [
    "import time,datetime\n",
    "from tqdm import tqdm\n",
    "# from multiprocessing import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T16:35:06.076674Z",
     "iopub.status.busy": "2023-02-17T16:35:06.076076Z",
     "iopub.status.idle": "2023-02-17T16:35:06.086653Z",
     "shell.execute_reply": "2023-02-17T16:35:06.085849Z",
     "shell.execute_reply.started": "2023-02-17T16:35:06.076633Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './' # ../input/amex-default-prediction\n",
    "data_dir_1 = './' # ../input/amex-default-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T01:34:04.176591Z",
     "iopub.status.busy": "2023-02-06T01:34:04.176153Z",
     "iopub.status.idle": "2023-02-06T01:46:14.267710Z",
     "shell.execute_reply": "2023-02-06T01:46:14.266262Z",
     "shell.execute_reply.started": "2023-02-06T01:34:04.176550Z"
    }
   },
   "outputs": [],
   "source": [
    "# # reading csv in chunks\n",
    "# train_data = pd.DataFrame()\n",
    "# with pd.read_csv(os.path.join(data_dir, 'train_data.csv'), chunksize=10**5) as reader:\n",
    "#     for counter, chunk in enumerate(reader):\n",
    "#         for column in chunk.columns:\n",
    "#             if str(chunk[column].dtype) == \"float64\":\n",
    "#                 chunk[column] = chunk[column].astype(np.float32)\n",
    "#             if str(chunk[column].dtype) == \"int64\":\n",
    "#                 chunk[column] = chunk[column].astype(np.int32)\n",
    "#         train_data = pd.concat([train_data, chunk])\n",
    "#         _ = gc.collect()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T04:06:15.457777Z",
     "iopub.status.busy": "2023-02-07T04:06:15.457227Z",
     "iopub.status.idle": "2023-02-07T04:06:15.493413Z",
     "shell.execute_reply": "2023-02-07T04:06:15.492097Z",
     "shell.execute_reply.started": "2023-02-07T04:06:15.457729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5531451 entries, 0 to 5531450\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: float32(185), float64(1), int8(2), object(2)\n",
      "memory usage: 3.9+ GB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T01:46:55.508346Z",
     "iopub.status.busy": "2023-02-06T01:46:55.507824Z",
     "iopub.status.idle": "2023-02-06T01:46:55.729382Z",
     "shell.execute_reply": "2023-02-06T01:46:55.727964Z",
     "shell.execute_reply.started": "2023-02-06T01:46:55.508302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'R', nan, 'U', '-1'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['D_64'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T01:47:25.222725Z",
     "iopub.status.busy": "2023-02-06T01:47:25.222292Z",
     "iopub.status.idle": "2023-02-06T01:47:25.232060Z",
     "shell.execute_reply": "2023-02-06T01:47:25.231053Z",
     "shell.execute_reply.started": "2023-02-06T01:47:25.222693Z"
    }
   },
   "outputs": [],
   "source": [
    "def denoise(df):\n",
    "    df['D_63'] = df['D_63'].apply(lambda t: {'CR':0, 'XZ':1, 'XM':2, 'CO':3, 'CL':4, 'XL':5}[t]).astype(np.int8)\n",
    "    df['D_64'] = df['D_64'].apply(lambda t: {np.nan:-1, 'O':0, '-1':1, 'R':2, 'U':3}[t]).astype(np.int8)\n",
    "#     df['D_64'] = df['D_64'].apply(lambda t: {None:-1, 'O':0, '-1':1, 'R':2, 'U':3}[t]).astype(np.int8)    \n",
    "    for col in tqdm(df.columns):\n",
    "        if col not in ['customer_ID','S_2','D_63','D_64']:\n",
    "            df[col] = np.floor(df[col]*100)\n",
    "            # Return the floor of the input, element-wise. \n",
    "            # np.floor(np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])) => array([-2., -2., -1.,  0.,  1.,  1.,  2.])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T01:47:28.545942Z",
     "iopub.status.busy": "2023-02-06T01:47:28.545538Z",
     "iopub.status.idle": "2023-02-06T01:47:39.613942Z",
     "shell.execute_reply": "2023-02-06T01:47:39.612596Z",
     "shell.execute_reply.started": "2023-02-06T01:47:28.545910Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:04<00:00, 45.86it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = denoise(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T01:47:58.344658Z",
     "iopub.status.busy": "2023-02-06T01:47:58.343289Z",
     "iopub.status.idle": "2023-02-06T01:49:40.555541Z",
     "shell.execute_reply": "2023-02-06T01:49:40.554111Z",
     "shell.execute_reply.started": "2023-02-06T01:47:58.344614Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.to_parquet(\"train_data\",  compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-07T04:12:05.734849Z",
     "iopub.status.busy": "2023-02-07T04:12:05.734372Z",
     "iopub.status.idle": "2023-02-07T04:12:05.817325Z",
     "shell.execute_reply": "2023-02-07T04:12:05.814981Z",
     "shell.execute_reply.started": "2023-02-07T04:12:05.734798Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T01:57:47.682444Z",
     "iopub.status.busy": "2023-02-06T01:57:47.681506Z",
     "iopub.status.idle": "2023-02-06T02:24:46.458789Z",
     "shell.execute_reply": "2023-02-06T02:24:46.457176Z",
     "shell.execute_reply.started": "2023-02-06T01:57:47.682387Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading csv in chunks\n",
    "# test_data = pd.DataFrame()\n",
    "# with pd.read_csv(os.path.join(data_dir, 'test_data.csv'), chunksize=10**5) as reader:\n",
    "#     for counter, chunk in enumerate(reader):\n",
    "#         for column in chunk.columns:\n",
    "#             if str(chunk[column].dtype) == \"float64\":\n",
    "#                 chunk[column] = chunk[column].astype(np.float32)\n",
    "#             if str(chunk[column].dtype) == \"int64\":\n",
    "#                 chunk[column] = chunk[column].astype(np.int32)\n",
    "#         test_data = pd.concat([test_data, chunk])\n",
    "#         _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T02:26:08.789890Z",
     "iopub.status.busy": "2023-02-06T02:26:08.789415Z",
     "iopub.status.idle": "2023-02-06T02:26:08.815976Z",
     "shell.execute_reply": "2023-02-06T02:26:08.814368Z",
     "shell.execute_reply.started": "2023-02-06T02:26:08.789851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11363762 entries, 0 to 11363761\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: float32(185), float64(1), int8(2), object(2)\n",
      "memory usage: 8.1+ GB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T02:25:40.720474Z",
     "iopub.status.busy": "2023-02-06T02:25:40.720018Z",
     "iopub.status.idle": "2023-02-06T02:26:03.406803Z",
     "shell.execute_reply": "2023-02-06T02:26:03.404583Z",
     "shell.execute_reply.started": "2023-02-06T02:25:40.720436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:07<00:00, 24.01it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = denoise(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T02:26:17.801414Z",
     "iopub.status.busy": "2023-02-06T02:26:17.800930Z",
     "iopub.status.idle": "2023-02-06T02:29:49.795699Z",
     "shell.execute_reply": "2023-02-06T02:29:49.794146Z",
     "shell.execute_reply.started": "2023-02-06T02:26:17.801375Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data.to_parquet(\"test_data\",  compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T01:55:00.484763Z",
     "iopub.status.busy": "2023-02-06T01:55:00.484259Z",
     "iopub.status.idle": "2023-02-06T01:55:00.696498Z",
     "shell.execute_reply": "2023-02-06T01:55:00.695152Z",
     "shell.execute_reply.started": "2023-02-06T01:55:00.484730Z"
    }
   },
   "outputs": [],
   "source": [
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_parquet(os.path.join(data_dir_1, 'train_data'))\n",
    "df_te = pd.read_parquet(os.path.join(data_dir_1, 'test_data'))\n",
    "df = pd.concat([df_tr,df_te], parallel = True,)\n",
    "del df_tr, df_te\n",
    "_ = gc.collect()\n",
    "df.write_parquet(f'train_test_data', compression='gzip',)\n",
    "del df\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16895213, 190)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:19.870110Z",
     "iopub.status.busy": "2023-02-16T05:28:19.869295Z",
     "iopub.status.idle": "2023-02-16T05:28:20.066675Z",
     "shell.execute_reply": "2023-02-16T05:28:20.065291Z",
     "shell.execute_reply.started": "2023-02-16T05:28:19.870049Z"
    }
   },
   "outputs": [],
   "source": [
    "# dfs[0][dfs[0]['customer_ID'] == '0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a']['rank_P_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T16:37:34.879630Z",
     "iopub.status.busy": "2023-02-17T16:37:34.878644Z",
     "iopub.status.idle": "2023-02-17T16:37:34.901714Z",
     "shell.execute_reply": "2023-02-17T16:37:34.900817Z",
     "shell.execute_reply.started": "2023-02-17T16:37:34.879590Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(dff,cols,is_drop=True):\n",
    "    # cols => ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "            # col, df[col].unique() => B_30, [  0. 200. 100.]\n",
    "    dummies = (\n",
    "        pd.get_dummies(\n",
    "            dff.select(cols),\n",
    "            columns = None\n",
    "        )\n",
    "    )            \n",
    "\n",
    "    # dropping one_hot_encoded null values (pola-rs also encode null values of a column).\n",
    "    _ = [dummies.drop_in_place(col) for col in dummies.columns if col[-4:] == 'null']\n",
    "    \n",
    "    dummies.columns = list(map(lambda x: 'oneHot_%s'%x, dummies.columns))        \n",
    "    \n",
    "        # dummies.columns => Index(['oneHot_B_30_0.0', 'oneHot_B_30_100.0', 'oneHot_B_30_200.0'], dtype='object')\n",
    "        # dummies['oneHot_B_30_0.0'].unique() => [1 0]\n",
    "        \n",
    "    # 'concat' is better for joining 'dummies with df' rather than 'join'. \n",
    "    # 'join' is not preferred as there is no need to match over a column, for dummies.\n",
    "    dff = pd.concat([dff,dummies], how = 'horizontal', parallel = True,)  \n",
    "    if is_drop:\n",
    "        dff = dff.drop(cols)\n",
    "    return dff\n",
    "\n",
    "def cat_feature(dff):\n",
    "    # type(df) => <class 'pandas.core.frame.DataFrame'>\n",
    "    one_hot_features = [col for col in df.columns if 'oneHot' in col]  \n",
    "    if lastk is None:\n",
    "        num_agg_df = dff.groupby(\"customer_ID\").agg([\n",
    "                            pd.col(one_hot_features).sum().suffix(\"_sum\"),\n",
    "                            pd.col(one_hot_features).mean().suffix(\"_mean\"),\n",
    "                            pd.col(one_hot_features).std().suffix(\"_std\"),\n",
    "                            pd.col(one_hot_features).last().suffix(\"_last\"), \n",
    "                        ])\n",
    "    else:\n",
    "        num_agg_df = dff.groupby(\"customer_ID\").agg([\n",
    "                            pd.col(one_hot_features).sum().suffix(\"_sum\"),\n",
    "                            pd.col(one_hot_features).mean().suffix(\"_mean\"),\n",
    "                            pd.col(one_hot_features).std().suffix(\"_std\"),\n",
    "                        ])            \n",
    "        # num_agg_df.columns =>\n",
    "        # Index(['oneHot_B_30_0.0_mean', 'oneHot_B_30_0.0_std', 'oneHot_B_30_0.0_sum',\n",
    "        #        'oneHot_B_30_0.0_last',        \n",
    "\n",
    "    if lastk is None:\n",
    "        # cat_features  => ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "        cat_agg_df = dff.groupby(\"customer_ID\").agg([\n",
    "                            pd.col(cat_features).last().suffix(\"_last\"),\n",
    "                            pd.col(cat_features).n_unique().suffix(\"_nunique\"),\n",
    "                        ])\n",
    "        # nunique => Return counts of unique elements in category for a particular customer_ID.\n",
    "    else:\n",
    "        cat_agg_df = dff.groupby(\"customer_ID\").agg([\n",
    "                            pd.col(cat_features).n_unique().suffix(\"_nunique\"),\n",
    "                        ])        \n",
    "        # cat_agg_df.columns =>\n",
    "        # Index(['B_30_last', 'B_30_nunique', 'B_38_last', 'B_38_nunique', 'D_114_last',\n",
    "        #         ...\n",
    "        #         ...\n",
    "        #        'D_68_nunique'],\n",
    "        #       dtype='object')    \n",
    "        \n",
    "    count_agg_df = dff.groupby(\"customer_ID\").agg([\n",
    "                        pd.col(['S_2']).count().suffix(\"_count\"),\n",
    "                    ])    \n",
    "        # S_2 (timestamp_column)\n",
    "        # count_agg_df.columns => Index(['S_2_count'], dtype='object')    \n",
    "    \n",
    "    dff = num_agg_df.join(cat_agg_df, on=\"customer_ID\", how=\"left\", ).join(count_agg_df, on=\"customer_ID\", how=\"left\", )    \n",
    "    print('cat feature shape after engineering', dff.shape )\n",
    "\n",
    "    return dff\n",
    "\n",
    "def num_feature(dff):\n",
    "    if num_features[0][:5] == 'rank_': \n",
    "        # num_features => ['rank_P_2', 'rank_D_39', 'rank_B_1', 'rank_B_2', 'rank_R_1', 'rank_S_3',......, 'rank_D_144', 'rank_D_145']        \n",
    "        dff = dff.groupby(\"customer_ID\").agg([\n",
    "                            pd.col(num_features).last().suffix(\"_last\"), \n",
    "                        ])        \n",
    "        # .agg(['last']) => keep last rank of each group.\n",
    "\n",
    "    else:\n",
    "        # num_features => ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', 'S_3',......, 'D_144', 'D_145']\n",
    "        # num_features[0], num_features[0][:5] => P_2, P_2         \n",
    "        if lastk is None:\n",
    "            dff = dff.groupby(\"customer_ID\").agg([\n",
    "                                pd.col(num_features).sum().suffix(\"_sum\"),\n",
    "                                pd.col(num_features).mean().suffix(\"_mean\"),\n",
    "                                pd.col(num_features).std().suffix(\"_std\"),\n",
    "                                pd.col(num_features).min().suffix(\"_min\"),                \n",
    "                                pd.col(num_features).max().suffix(\"_max\"),                \n",
    "                                pd.col(num_features).last().suffix(\"_last\"), \n",
    "                            ])            \n",
    "        else:\n",
    "            dff = dff.groupby(\"customer_ID\").agg([\n",
    "                                pd.col(num_features).sum().suffix(\"_sum\"),\n",
    "                                pd.col(num_features).mean().suffix(\"_mean\"),\n",
    "                                pd.col(num_features).std().suffix(\"_std\"),\n",
    "                                pd.col(num_features).min().suffix(\"_min\"),                \n",
    "                                pd.col(num_features).max().suffix(\"_max\"),                \n",
    "                            ])            \n",
    "\n",
    "    if num_features[0][:5] != 'rank_':\n",
    "        numeric_cols = [col for col in dff.columns if col != 'customer_ID']\n",
    "        # num_agg_df; returns 'dataframe with operated columns only'.\n",
    "        dff = dff.with_columns([\n",
    "                            pd.col(numeric_cols).apply(lambda x: x // 0.01),\n",
    "                        ])   # e.g., mean 92.846153 convert 9284.0\n",
    "            # 92.846153 // 0.01 => 9284.0, 92.846153 / 0.01 => 9284.6153\n",
    "            \n",
    "    print('num feature shape after engineering', dff.shape )\n",
    "\n",
    "    return dff\n",
    "\n",
    "def diff_feature(dff):\n",
    "#     diff_num_features = [f'diff_{col}' for col in num_features]            \n",
    "    \n",
    "    dff =  dff[['customer_ID']+num_features].select([\n",
    "               pd.col('customer_ID'),\n",
    "               pd.col(num_features).diff().over('customer_ID').prefix('diff_'),\n",
    "            ])                    \n",
    "    # .diff() => difference of a DataFrame element compared with another element in the 'previous row' (default). \n",
    "    # .prefix() => column labels are prefixed.        \n",
    "    \n",
    "    num_cols = [col for col in dff.columns if col != 'customer_ID']    \n",
    "        \n",
    "    if lastk is None:        \n",
    "        dff = dff.groupby(\"customer_ID\").agg([\n",
    "                            pd.col(num_cols).sum().suffix(\"_sum\"),\n",
    "                            pd.col(num_cols).mean().suffix(\"_mean\"),\n",
    "                            pd.col(num_cols).std().suffix(\"_std\"),\n",
    "                            pd.col(num_cols).min().suffix(\"_min\"),                \n",
    "                            pd.col(num_cols).max().suffix(\"_max\"),                \n",
    "                            pd.col(num_cols).last().suffix(\"_last\"),            \n",
    "                        ])        \n",
    "        \n",
    "    else:\n",
    "        dff = dff.groupby(\"customer_ID\").agg([\n",
    "                            pd.col(num_cols).sum().suffix(\"_sum\"),\n",
    "                            pd.col(num_cols).mean().suffix(\"_mean\"),\n",
    "                            pd.col(num_cols).std().suffix(\"_std\"),\n",
    "                            pd.col(num_cols).min().suffix(\"_min\"),                \n",
    "                            pd.col(num_cols).max().suffix(\"_max\"),       \n",
    "                        ])        \n",
    "\n",
    "    num_cols = [col for col in dff.columns if col != 'customer_ID']        \n",
    "    dff = dff.with_columns([\n",
    "                        pd.col(num_cols).apply(lambda x: x // 0.01),\n",
    "                    ])        \n",
    "\n",
    "    print('diff feature shape after engineering', dff.shape )\n",
    "\n",
    "    return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T07:10:58.602849Z",
     "iopub.status.busy": "2023-02-06T07:10:58.602351Z",
     "iopub.status.idle": "2023-02-06T07:10:58.642764Z",
     "shell.execute_reply": "2023-02-06T07:10:58.640385Z",
     "shell.execute_reply.started": "2023-02-06T07:10:58.602810Z"
    }
   },
   "outputs": [],
   "source": [
    "# # creates new column 'assign'. trick; remove '[' and ']' symbols.\n",
    "# df = df.select( \n",
    "#         diff_ = pd.col(num_features).diff()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-17T16:35:35.881291Z",
     "iopub.status.busy": "2023-02-17T16:35:35.880836Z",
     "iopub.status.idle": "2023-02-17T16:36:09.975465Z",
     "shell.execute_reply": "2023-02-17T16:36:09.974514Z",
     "shell.execute_reply.started": "2023-02-17T16:35:35.881256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num feature shape after engineering (1383534, 178)\n",
      "num feature shape after engineering (1383534, 1063)\n",
      "all df shape (16895213, 190)\n",
      "last 3 shape (2756121, 190)\n",
      "cat feature shape after engineering (1383534, 148)\n",
      "num feature shape after engineering (1383534, 886)\n",
      "diff feature shape after engineering (1383534, 886)\n",
      "all df shape (16895213, 190)\n",
      "last 6 shape (6790883, 190)\n",
      "num feature shape after engineering (1383534, 886)\n"
     ]
    }
   ],
   "source": [
    "n_cpu = 12 # 16\n",
    "transform = [['','rank_','ym_rank_'],[''],['']]\n",
    "# transform = [['rank_','ym_rank_'],[''],['']] # for debug\n",
    "\n",
    "\n",
    "for li, lastk in enumerate([None,3,6]):\n",
    "    for prefix in transform[li]:\n",
    "# #         df = pd.read_feather(f'./input/train.feather').append(pd.read_feather(f'./input/test.feather'), ignore_index=True).reset_index(drop=True)\n",
    "        df = pd.read_parquet(os.path.join(data_dir_1, 'train_test_data'))\n",
    "#         df = pd.read_parquet(os.path.join(data_dir_1, 'train_data')) # for debugging    \n",
    "\n",
    "        all_cols = [c for c in list(df.columns) if c not in ['customer_ID', 'S_2']]\n",
    "         # leaving 'customer_ID' and 'S_2'\n",
    "        \n",
    "        cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "        num_features = [col for col in all_cols if col not in cat_features]\n",
    "          # num_features => ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', ...,'S_17', ..., 'D_142', 'D_143', 'D_144', 'D_145']\n",
    "          #  D_136, D_137, D_138, D_142 etc. have NaN values.\n",
    "        \n",
    "        for col in [col for col in df.columns if 'S_' in col or 'P_' in col]:\n",
    "            if col != 'S_2':\n",
    "                # with_columns; returns whole dataframe instead of 'dataframe with operated columns only'.\n",
    "                df = df.with_columns([\n",
    "                            pd.col(col).fill_null(0),\n",
    "                        ])                \n",
    "\n",
    "        if lastk is not None:\n",
    "            prefix = f'last{lastk}_' + prefix\n",
    "            print('all df shape',df.shape)            \n",
    "            df = df.with_columns(\n",
    "                        rank = pd.col('S_2').rank().reverse().over('customer_ID') \n",
    "                        # .reverse; current date have low rank\n",
    "                    ).filter(pd.col('rank') < lastk).drop('rank') # keep top 3 rank or top 6 rank.           \n",
    "            \n",
    "            print(f'last {lastk} shape',df.shape)\n",
    "                    \n",
    "        if prefix == 'rank_':           \n",
    "            # rank elements in each group. before and after no change in row count.\n",
    "            df = df[['customer_ID']+num_features].select([\n",
    "                        pd.col('customer_ID'),                \n",
    "                       (pd.col(num_features).rank(method='average').over('customer_ID') / pd.col(num_features).count().over('customer_ID')).prefix('rank_'),\n",
    "                    ])\n",
    "                # .rank(pct=True) => Computes percentage rank of values i.e., arrange the ranks between '0.' and '1.'.\n",
    "                # e.g. => elements - 4, 4, 7, 7, 5, 6\n",
    "                # elements (sort) - 4, 4, 5, 6, 7, 7\n",
    "                # ranks    - 1, 2, 3, 4, 5, 6\n",
    "                # ranks (again) - 1.5, 1.5, 3, 4, 5.5, 5.5 => ranks of repeated items is averaged.\n",
    "                # ranks (pct) - 0.250000, 0.250000, 0.500000, 0.666667, 0.916667, 0.916667            \n",
    "            num_features = [f'rank_{col}' for col in num_features]                \n",
    "                \n",
    "        if prefix == 'ym_rank_': # ym => year-month\n",
    "            df = df.with_columns(\n",
    "                        S_2 = pd.col(['S_2']).apply(lambda x:x[:7]) # pick year-month from date column 'S_2'.\n",
    "                    )                        \n",
    "\n",
    "            # '.over' behaves similar to 'groupby'.\n",
    "            df = df[['customer_ID','S_2']+num_features].select([\n",
    "                        pd.col('customer_ID'),\n",
    "                        (pd.col(num_features).rank(method='average').over(\"S_2\") / pd.col(num_features).count().over(\"S_2\")).prefix('ym_rank_'),\n",
    "                    ])            \n",
    "            num_features = [f'ym_rank_{col}' for col in num_features]\n",
    "\n",
    "        if prefix in ['','last3_']:\n",
    "            df = one_hot_encoding(df,cat_features,False)        \n",
    "\n",
    "#         vc = df['customer_ID'].value_counts(sort=False).select([\n",
    "#                 pd.cumsum(\"counts\")\n",
    "#             ])        \n",
    "            \n",
    "#         batch_size = int(np.ceil(len(vc) / n_cpu))        \n",
    "#             # np.ceil(np.array([-1.7, -0.2, 0.2, 1.7, 2.0])) => array([-1., -0.,  1.,  2.,  2.]) \n",
    "            \n",
    "#         dfs = []\n",
    "#         start = 0\n",
    "#         # make list of dataframe batches in accordance to cpu cores.\n",
    "#         for i in range(min(n_cpu,int(np.ceil(len(vc) / batch_size)))):                \n",
    "#             vc_ = vc[i*batch_size:(i+1)*batch_size]\n",
    "#                 # vc_ =>\n",
    "#                 # df['customer_ID'].value_counts(sort=False).cumsum() => \n",
    "#                 # 0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a         13\n",
    "#                 # 00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5         26\n",
    "#                 # ...\n",
    "#                 # fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eaba8b115f71cab04681    5531451\n",
    "#             dfs.append(df[start:vc_[-1].item()])\n",
    "#                 # df[start:vc_[-1]] => df[0:5531451]\n",
    "#             start = vc_[-1].item()\n",
    "\n",
    "#         pool = ThreadPool(n_cpu)\n",
    "\n",
    "        if prefix in ['','last3_']:\n",
    "#             cat_feature_df = pd.concat(pool.map(cat_feature,tqdm(dfs,desc='cat_feature'))).reset_index(drop=True)\n",
    "#             cat_feature_df.to_feather(f'./input/{prefix}cat_feature.feather')\n",
    "\n",
    "            cat_feature_df = cat_feature(df)\n",
    "#             cat_feature_df = pd.concat([cat_feature(dfs[l]) for l in range(len(dfs))], how = 'vertical', parallel = True,)\n",
    "            cat_feature_df.write_parquet(f'{prefix}cat_feature', compression='gzip')\n",
    "                # tqdm(dfs,desc='cat_feature') => desc; add message to tqdm progress.\n",
    "                # pool.map(function_name, arguments)\n",
    "            del cat_feature_df\n",
    "            _ = gc.collect()\n",
    "\n",
    "        if prefix in ['','last3_','last6_','rank_','ym_rank_']:\n",
    "#             num_feature_df = pd.concat(pool.map(num_feature,tqdm(dfs,desc='num_feature'))).reset_index(drop=True)\n",
    "#             num_feature_df.to_feather(f'./input/{prefix}num_feature.feather')\n",
    "\n",
    "            num_feature_df = num_feature(df)\n",
    "#             num_feature_df = pd.concat([num_feature(dfs[l]) for l in range(len(dfs))], how = 'vertical', parallel = True,)\n",
    "            num_feature_df.write_parquet(f'{prefix}num_feature', compression='gzip')\n",
    "            del num_feature_df\n",
    "            _ = gc.collect()        \n",
    "\n",
    "        if prefix in ['','last3_']:\n",
    "#             diff_feature_df = pd.concat(pool.map(diff_feature,tqdm(dfs,desc='diff_feature'))).reset_index(drop=True)\n",
    "#             diff_feature_df.to_feather(f'./input/{prefix}diff_feature.feather')\n",
    "\n",
    "            diff_feature_df = diff_feature(df)\n",
    "#             diff_feature_df = pd.concat([diff_feature(dfs[l]) for l in range(len(dfs))], how = 'vertical', parallel = True,)\n",
    "            diff_feature_df.write_parquet(f'{prefix}diff_feature', compression='gzip') \n",
    "            del diff_feature_df\n",
    "            _ = gc.collect()        \n",
    "\n",
    "#         pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep copies of ['cat','num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num'] files to the inside of folder 'extra'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
