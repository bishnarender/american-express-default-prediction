{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import gc,os,random\n",
    "import time,datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "# root = args.root\n",
    "# seed = args.seed\n",
    "# remark = args.remark\n",
    "# save_dir = args.save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b53ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graphviz\n",
    "# from torchview import draw_graph\n",
    "# graphviz.set_jupyter_format('png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a2e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc4f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader,RandomSampler\n",
    "from typing import Optional, Callable, List\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b21886",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0fe2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='.'\n",
    "remark=''\n",
    "save_dir=''\n",
    "seed=42\n",
    "id_name = 'customer_ID'\n",
    "label_name = 'target'\n",
    "num_workers = 11\n",
    "use_amp = False\n",
    "do_train = True\n",
    "\n",
    "eps = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34aea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f0296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available gpus: [0]\n"
     ]
    }
   ],
   "source": [
    "gpus = list(range(torch.cuda.device_count()))\n",
    "print('available gpus:',gpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22600bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3075294",
   "metadata": {},
   "source": [
    "##  Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d57ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65\n",
    "def reduce_mem_usage(props):\n",
    "#     start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "#     print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "#     NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "#             # Print current column type\n",
    "#             print(\"******************************\")\n",
    "#             print(\"Column: \",col)\n",
    "#             print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "#             # Integer does not support NA, therefore, NA needs to be filled\n",
    "#             if not np.isfinite(props[col]).all(): \n",
    "#                 NAlist.append(col)\n",
    "#                 props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column dtype is int.\n",
    "            if 'int' in props[col].dtype.name:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "#             # Print new column type\n",
    "#             print(\"dtype after: \",props[col].dtype)\n",
    "#             print(\"******************************\")\n",
    "    \n",
    "#     # Print final result\n",
    "#     print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "#     mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "#     print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "#     print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "#     return props, NAlist\n",
    "    return props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f91f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df,cols,is_drop=True):\n",
    "    for col in cols:\n",
    "#         print('one hot encoding:',col)\n",
    "        dummies = pd.get_dummies(pd.Series(df[col]),prefix='oneHot_%s'%col)\n",
    "        df = pd.concat([df,dummies],axis=1)\n",
    "    if is_drop:\n",
    "        df.drop(cols,axis=1,inplace=True)\n",
    "    return df\n",
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "\n",
    "\n",
    "df = pd.read_parquet(f'{root}/train_test_data')\n",
    "df = df.drop(['S_2'],axis=1)\n",
    "df = one_hot_encoding(df,cat_features,True)\n",
    "\n",
    "for col in tqdm(df.columns):\n",
    "    if col not in ['customer_ID','S_2']:\n",
    "        df[col] /= 100\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "df = reduce_mem_usage(df)\n",
    "df.to_parquet(f'{root}/nn_train_test_data',  compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a4e246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8123d5f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9174c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and saving series features (polars library).\n",
    "df = pl.read_parquet(f'{root}/nn_train_test_data')\n",
    "train_y =  pl.read_csv(f'{root}/train_labels.csv').sort(by=\"customer_ID\", reverse=False)\n",
    "df.filter(~pl.col(\"customer_ID\").is_in(train_y[\"customer_ID\"]),\n",
    "         ).sort(by=\"customer_ID\", reverse=False).write_parquet(f'{root}/nn_series_test_feature', compression='gzip')\n",
    "df.filter(pl.col(\"customer_ID\").is_in(train_y[\"customer_ID\"]),\n",
    "         ).sort(by=\"customer_ID\", reverse=False).write_parquet(f'{root}/nn_series_train_feature', compression='gzip')\n",
    "train_y.write_parquet(f'{root}/train_labels', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1504b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bdc28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and saving manual features (polars library).\n",
    "df = pl.read_parquet(f'{root}/nn_all_feature')\n",
    "train_y =  pl.read_parquet(f'{root}/train_labels')\n",
    "df = df.filter(~pl.col(\"customer_ID\").is_in(train_y[\"customer_ID\"]))\n",
    "df = df.sort(by=\"customer_ID\", reverse=False)\n",
    "df.write_parquet(f'{root}/nn_manual_test_feature', compression='gzip')\n",
    "\n",
    "df = pl.read_parquet(f'{root}/nn_all_feature')\n",
    "df = df.filter(pl.col(\"customer_ID\").is_in(train_y[\"customer_ID\"]))\n",
    "df = df.sort(by=\"customer_ID\", reverse=False)\n",
    "df.write_parquet(f'{root}/nn_manual_train_feature', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b6fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb2ea28",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5e8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a01a4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amodel(nn.Module):\n",
    "    def __init__(self, series_dim, feature_dim, target_num, hidden_num, hidden_dim, drop_rate=0.5, use_series_oof=False):\n",
    "        super(Amodel, self).__init__()\n",
    "        self.use_series_oof = use_series_oof\n",
    "\n",
    "        self.input_series_block = nn.Sequential(\n",
    "                                        nn.Linear(series_dim, hidden_dim)\n",
    "                                        ,nn.LayerNorm(hidden_dim)\n",
    "                                        )\n",
    "        self.input_feature_block = nn.Sequential(\n",
    "                                        nn.Linear(feature_dim, hidden_dim)\n",
    "                                        ,nn.BatchNorm1d(hidden_dim)\n",
    "                                        ,nn.LeakyReLU()\n",
    "                                        )\n",
    "        self.gru_series = nn.GRU(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.hidden_feature_block = []\n",
    "        for h in range(hidden_num-1):\n",
    "            # extend; add element/block/dataframe/etc. to list as previous element/block/dataframe/etc..\n",
    "            self.hidden_feature_block.extend([\n",
    "                                     nn.Linear(hidden_dim, hidden_dim)\n",
    "                                     ,nn.BatchNorm1d(hidden_dim)\n",
    "                                     ,nn.Dropout(drop_rate)\n",
    "                                     ,nn.LeakyReLU()\n",
    "                                     ])\n",
    "        self.hidden_feature_block = nn.Sequential(*self.hidden_feature_block)\n",
    "\n",
    "        self.output_block = nn.Sequential(\n",
    "                                         nn.Linear(3*hidden_dim if use_series_oof else 2*hidden_dim, 1*hidden_dim)\n",
    "                                         ,nn.LeakyReLU()\n",
    "                                         ,nn.Linear(1*hidden_dim, 1*hidden_dim)\n",
    "                                         ,nn.LeakyReLU()                                         \n",
    "                                         ,nn.Linear(1*hidden_dim, target_num)\n",
    "                                         ,nn.Sigmoid()\n",
    "                                         )\n",
    "\n",
    "    def batch_gru(self,series,mask):\n",
    "            # series.shape, mask.shape => torch.Size([2, 13, 128]), torch.Size([2, 13]) => (2 is batch_size)\n",
    "        node_num = mask.sum(dim=-1).detach().cpu()\n",
    "            # node_num, node_num.shape => tensor([13., 13.]), torch.Size([2])\n",
    "\n",
    "        # All RNN modules accept packed sequences as inputs.\n",
    "        # Packs a Tensor (containing padded sequences of variable length) by removing pads. \n",
    "        # i.e., for both batches, we have [13,128] and [13,128] data. And we mentioned in node_num [13,13] that -\n",
    "        # - the sequence length of both batch elements is '13'. So, pack all data and here is nothing to remove.\n",
    "        pack = nn.utils.rnn.pack_padded_sequence(series, node_num, batch_first=True, enforce_sorted=False)\n",
    "            # pack.data.shape, pack.batch_sizes.shape => torch.Size([26, 128]), torch.Size([13])\n",
    "            # batch_sizes => tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) => ( 13*[2,128] = [26,128] )\n",
    "            # pack.sorted_indices, pack.unsorted_indices => tensor([0, 1]), tensor([0, 1])\n",
    "            # sorted indices [0,1] i.e., order of packing for each batch of 2 => \n",
    "            #  1st batch => [0,0,:] (torch.Size([13, 128])) and [1,0,:] (torch.Size([13, 128]))\n",
    "            #  2nd batch => [0,1,:] (torch.Size([13, 128])) and [1,1,:] (torch.Size([13, 128]))\n",
    "            #  3rd batch => [0,2,:] (torch.Size([13, 128])) and [1,2,:] (torch.Size([13, 128]))\n",
    "            #  i.e., to prepare 'pack batch' pick one element from every original batch.\n",
    "\n",
    "        message, hidden = self.gru_series(pack)            \n",
    "            # message.data.shape, hidden.shape => torch.Size([26, 256]) torch.Size([2, 2, 128])\n",
    "            # message.batch_sizes => tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
    "            # hidden => the last hidden state of GRU after whole pack passed.\n",
    "            # message => contains hidden state value at the time of every input.\n",
    "        \n",
    "        pooling_feature = []\n",
    "        for i,n in enumerate(node_num.numpy()):\n",
    "            n = int(n)\n",
    "                # n => 13\n",
    "            bi = 0\n",
    "            \n",
    "            # unsorted_indices is for retrieval in the same order as opted during packing.            \n",
    "            si = message.unsorted_indices[i] \n",
    "                # si => tensor(0)\n",
    "            for k in range(n):\n",
    "                if k == n-1:\n",
    "                    sample_feature = message.data[bi+si]\n",
    "                        # bi+si => tensor(24)                    \n",
    "                        # sample_feature.shape => torch.Size([256])\n",
    "                bi = bi + message.batch_sizes[k]\n",
    "            pooling_feature.append(sample_feature)\n",
    "        return torch.stack(pooling_feature,0)\n",
    "\n",
    "    def forward(self, data):\n",
    "            # data['batch_series'].shape => torch.Size([2, 13, 223]) => torch.Size([batch_size, 13, 223])\n",
    "        x1 = self.input_series_block(data['batch_series'])\n",
    "            # x1.shape => torch.Size([2, 13, 128]) \n",
    "        x1 = self.batch_gru(x1,data['batch_mask'])\n",
    "            # x1.shape => torch.Size([2, 256])\n",
    "\n",
    "        if self.use_series_oof:\n",
    "            x2 = self.input_feature_block(data['batch_feature'])\n",
    "                # x2.shape =>  torch.Size([2, 128])\n",
    "            x2 = self.hidden_feature_block(x2)\n",
    "                # x2.shape => torch.Size([2, 128])\n",
    "            x = torch.cat([x1,x2],axis=1)\n",
    "                # x.shape => torch.Size([2, 384])\n",
    "            y = self.output_block(x)\n",
    "                # y.shape => torch.Size([2, 1])\n",
    "        else:\n",
    "            y = self.output_block(x1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25f57d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = TaskDataset(df,f,[series_idx.values[i] for i in range(2)],y)\n",
    "# train_dataloader = DataLoader(train_dataset,batch_size=2,shuffle=True, drop_last=True, collate_fn=train_dataset.collate_fn,num_workers=11)\n",
    "# m = Amodel(223,(6372+13)*2,1,3,128,use_series_oof=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f21c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_dataloader:\n",
    "#     draw_graph(m, input_data = [data], expand_nested=True, save_graph=True, device='cpu').visual_graph\n",
    "#     m.forward(data)    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98431292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893efe0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3553c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskDataset:\n",
    "    def __init__(self,df_series,df_feature,uidxs,df_y=None):\n",
    "        self.df_series = df_series\n",
    "        self.df_feature = df_feature\n",
    "        self.df_y = df_y\n",
    "        self.uidxs = uidxs\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.uidxs))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # (i1 => index_start, i2 => index_end, idx => customer_ID index) in series_idx array.\n",
    "        i1,i2,idx = self.uidxs[index]\n",
    "        series = self.df_series.iloc[i1:i2+1,1:].values # converting all 'iloc values' to array leaving customer_ID.\n",
    "            # series.shape => (13, 223)        \n",
    "\n",
    "        if len(series.shape) == 1:\n",
    "                # series.shape => (223,) \n",
    "            series = series.reshape((-1,)+series.shape[-1:])\n",
    "                # series.shape => (1, 223)\n",
    "                \n",
    "#         series_ = series.copy()\n",
    "#             # series_ => \n",
    "#             # [[0.93 0.   0.   ... 0.   0.   0.01]\n",
    "#             #  [0.93 0.   0.   ... 0.   0.   0.01]\n",
    "#             #  [0.95 0.09 0.02 ... 0.   0.   0.01]\n",
    "#             # ...\n",
    "#         series_[series_!=0] = 1.0 - series_[series_!=0] + 0.001\n",
    "#             # series_        \n",
    "#             # [[0.07099999 0.         0.         ... 0.         0.         0.991     ]\n",
    "#             #  [0.07099999 0.         0.         ... 0.         0.         0.991     ]\n",
    "#             #  [0.05100001 0.91099995 0.981      ... 0.         0.         0.991     ]\n",
    "#             # ...        \n",
    "\n",
    "        feature = self.df_feature.loc[idx].values[1:]\n",
    "            # feature.shape => (6385,)\n",
    "        feature_ = feature.copy()\n",
    "        feature_[feature_!=0] = 1.0 - feature_[feature_!=0] + 0.001\n",
    "            # feature.shape => (6385,)        \n",
    "        if self.df_y is not None:\n",
    "            label = self.df_y.loc[idx,[label_name]].values\n",
    "            return {\n",
    "                    'SERIES': series,#np.concatenate([series,series_],axis=1),\n",
    "                    'FEATURE': np.concatenate([feature,feature_]), \n",
    "                        # np.concatenate([feature,feature_]).shape => (12770,)\n",
    "                    'LABEL': label,\n",
    "                    }\n",
    "        else:\n",
    "            return {\n",
    "                    'SERIES': series,#np.concatenate([series,series_],axis=1),\n",
    "                    'FEATURE': np.concatenate([feature,feature_]),\n",
    "                    }\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Padding to same size.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = len(batch)\n",
    "        batch_series = torch.zeros((batch_size, 13, batch[0]['SERIES'].shape[1])) # (1,13,223)\n",
    "        batch_mask = torch.zeros((batch_size, 13)) \n",
    "        batch_feature = torch.zeros((batch_size, batch[0]['FEATURE'].shape[0])) # (1,12770)\n",
    "        batch_y = torch.zeros((batch_size, 1))\n",
    "\n",
    "        for i, item in enumerate(batch):\n",
    "            v = item['SERIES'].astype(np.float32)\n",
    "            batch_series[i, :v.shape[0], :] = torch.tensor(v)#.float()\n",
    "            batch_mask[i,:v.shape[0]] = 1.0\n",
    "            v = item['FEATURE'].astype(np.float32)\n",
    "            batch_feature[i] = torch.tensor(v)#.float()\n",
    "            if self.df_y is not None:\n",
    "                v = item['LABEL'].astype(np.float32)\n",
    "                batch_y[i] = torch.tensor(v)#.float()\n",
    "\n",
    "        return {'batch_series':batch_series,'batch_mask':batch_mask,'batch_feature':batch_feature,'batch_y':batch_y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca51db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric(labels,preds):\n",
    "    return amex_metric_mod(labels,preds)\n",
    "\n",
    "def amex_metric_mod(y_true, y_pred):\n",
    "        # y_true => \n",
    "        # 0          0\n",
    "        # 1          0\n",
    "        # 2          0\n",
    "        #           ..\n",
    "        # 5531449    0\n",
    "        # 5531450    0\n",
    "        # Name: target, Length: 2765213, dtype: int64\n",
    "        \n",
    "        # y_pred => [0.0018315  0.00164183 0.00174071 ... 0.00212767 0.00309472]\n",
    "    labels     = np.transpose(np.array([y_true, y_pred]))    \n",
    "\n",
    "    labels     = labels[labels[:, 1].argsort()[::-1]]\n",
    "        # .argsort() => Return the integer indices that would sort the Series values.\n",
    "        # .argsort()[::-1] => reverse the sorted indices.\n",
    "        # labels => \n",
    "        # [[1.00000000e+00 9.97522107e-01]\n",
    "        # [1.00000000e+00 9.97491614e-01]\n",
    "        # ...\n",
    "        # [0.00000000e+00 8.86596095e-05]\n",
    "        # [0.00000000e+00 8.82186859e-05]]\n",
    "        # labels[:,0] denotes originl targets, labels[:,1] denotes predicted targets.\n",
    "        \n",
    "    weights    = np.where(labels[:,0]==0, 20, 1)\n",
    "        # weights => [ 1  1  1 ... 20 20 20]\n",
    "        # np.cumsum(weights) => [       1        2        3 ... 42218046 42218066 42218086]\n",
    "        # np.sum(weights) => 42218086\n",
    "    \n",
    "    cut_vals   = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "        # cut_vals =>     \n",
    "        # [[1.         0.99752211]\n",
    "        #  [1.         0.99749161]\n",
    "        #  ...\n",
    "        #  [1.         0.69189778]\n",
    "        #  [0.         0.69189679]]    \n",
    "    \n",
    "    \n",
    "    top_four   = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "        # np.sum(cut_vals[:,0]) => sum of actual labels from cut_vals array.\n",
    "        # np.sum(labels[:,0]) => sum of actual labels from labels array.\n",
    "        # np.sum(cut_vals[:,0]), np.sum(labels[:,0]) => 388343.0, 688746.0    \n",
    "        # top_four => 0.5638406611435856\n",
    "\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels         = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels         = labels[labels[:, i].argsort()[::-1]] # i = 1,0\n",
    "            # when i==0 then labels => \n",
    "            # [[1.         0.67152673]\n",
    "            #  [1.         0.6808289 ]\n",
    "            #  ...\n",
    "            #  [0.         0.5255297 ]\n",
    "            #  [0.         0.0018315 ]]            \n",
    "        weights        = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random  = np.cumsum(weights / np.sum(weights))\n",
    "            # weight_random =>\n",
    "            # [2.36865309e-08 4.73730619e-08 7.10595928e-08 ... 9.99999052e-01\n",
    "            #  9.99999526e-01 1.00000000e+00]        \n",
    "        total_pos      = np.sum(labels[:, 0] *  weights)\n",
    "            # total_pos => 688746.0\n",
    "        cum_pos_found  = np.cumsum(labels[:, 0] * weights)\n",
    "            # cum_pos_found => \n",
    "            # [1.00000e+00 2.00000e+00 3.00000e+00 ... 6.88746e+05 6.88746e+05\n",
    "            #  6.88746e+05]        \n",
    "        lorentz        = cum_pos_found / total_pos\n",
    "            # lorentz =>        \n",
    "            # [1.45191406e-06 2.90382812e-06 4.35574217e-06 ... 1.00000000e+00\n",
    "            #  1.00000000e+00 1.00000000e+00]        \n",
    "        gini[i]        = np.sum((lorentz - weight_random) * weights)\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def Write_log(logFile,text,isPrint=False):\n",
    "    if isPrint:\n",
    "        print(text)\n",
    "    logFile.write(text)\n",
    "    logFile.write('\\n')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d74baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerBase(object):\n",
    "    def __init__(self):\n",
    "        self._is_load_best_weight = True\n",
    "        self._is_load_best_optim = True\n",
    "        self._is_freeze_bn=False\n",
    "        self._is_adjust_lr = True\n",
    "        self._lr = 0.01\n",
    "        self._cur_optimizer = None\n",
    "\n",
    "    def schedule(self, net, epoch, epochs, **kwargs):\n",
    "        raise Exception('Did not implemented')\n",
    "\n",
    "    def step(self, net, epoch, epochs):\n",
    "        optimizer, lr = self.schedule(net, epoch, epochs)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        lr_list = []\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr_list += [param_group['lr']]\n",
    "        return lr_list\n",
    "\n",
    "    def is_load_best_weight(self):\n",
    "        return self._is_load_best_weight\n",
    "\n",
    "    def is_load_best_optim(self):\n",
    "        return self._is_load_best_optim\n",
    "\n",
    "    def is_freeze_bn(self):\n",
    "        return self._is_freeze_bn\n",
    "\n",
    "    def reset(self):\n",
    "        self._is_load_best_weight = True\n",
    "        self._load_best_optim = True\n",
    "        self._is_freeze_bn = False\n",
    "\n",
    "    def is_adjust_lr(self):\n",
    "        return self._is_adjust_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee2900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam12(SchedulerBase):\n",
    "    def __init__(self, params_list=None):\n",
    "        super().__init__()\n",
    "        self._lr = 100e-6 # 100e-6\n",
    "        self._cur_optimizer = None\n",
    "        self.params_list=params_list\n",
    "\n",
    "    def schedule(self, net, epoch, epochs, **kwargs):\n",
    "        lr = 100e-5 # 100e-5\n",
    "        if epoch > 4:\n",
    "            lr = 100e-6 # 100e-6\n",
    "        if epoch > 8:\n",
    "            lr = 100e-7 # 100e-7\n",
    "        # if epoch > 9:\n",
    "        #     lr = 1e-5\n",
    "        # if epoch > 12:\n",
    "        #     lr = 1e-5\n",
    "        self._lr = lr\n",
    "        if self._cur_optimizer is None:\n",
    "            self._cur_optimizer = optim.Adam(net.parameters(), lr=lr)#, eps=1e-5, weight_decay=0.001\n",
    "        return self._cur_optimizer, self._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac76c295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414086db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_train_and_predict(train, test, model_class, config, use_series_oof, logit=False, output_root='./output/', run_id=None):\n",
    "    if not run_id:\n",
    "        run_id = 'run_nn_' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        while os.path.exists(output_root+run_id+'/'):\n",
    "            time.sleep(1)\n",
    "            run_id = 'run_nn_' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = output_root + f'{save_dir}/'\n",
    "    else:\n",
    "        output_path = output_root + run_id + '/'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "        \n",
    "#     os.system(f'cp ./*.py {output_path}')\n",
    "    \n",
    "    feature_name = config['feature_name']\n",
    "    obj_max = config['obj_max']\n",
    "    epochs = config['epochs']\n",
    "    smoothing = config['smoothing']\n",
    "    patience = config['patience']\n",
    "    lr = config['lr']\n",
    "    batch_size = config['batch_size']\n",
    "    folds = config['folds']\n",
    "    seed = config['seed']\n",
    "    \n",
    "    if train is not None:\n",
    "        train_series,train_feature,train_y,train_series_idx = train\n",
    "\n",
    "        oof = train_y[[id_name]]\n",
    "        oof['fold'] = -1\n",
    "        oof[label_name] = 0.0\n",
    "        oof[label_name] = oof[label_name].astype(np.float32)\n",
    "    else:\n",
    "        oof = None\n",
    "\n",
    "    if train is not None:\n",
    "        log = open(output_path + 'train.log','w',buffering=1)\n",
    "        log.write(str(config)+'\\n')\n",
    "\n",
    "        all_valid_metric = []\n",
    "\n",
    "        skf = StratifiedKFold(n_splits = folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        model_num = 0\n",
    "        train_folds = []\n",
    "\n",
    "        for fold, (trn_index, val_index) in enumerate(skf.split(train_y,train_y[label_name])):\n",
    "\n",
    "            train_dataset = TaskDataset(train_series,train_feature,[train_series_idx[i] for i in trn_index],train_y)\n",
    "            train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, drop_last=True, collate_fn=train_dataset.collate_fn,num_workers=num_workers)\n",
    "            valid_dataset = TaskDataset(train_series,train_feature,[train_series_idx[i] for i in val_index],train_y)\n",
    "            valid_dataloader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=False, drop_last=False, collate_fn=valid_dataset.collate_fn,num_workers=num_workers)\n",
    "\n",
    "            model = model_class(223,(6372+13)*2,1,3,128,use_series_oof=use_series_oof) # 6375+13\n",
    "            scheduler = Adam12()\n",
    "\n",
    "            model.cuda()\n",
    "            if use_amp:\n",
    "                scaler = amp.GradScaler()\n",
    "            optimizer = scheduler.schedule(model, 0, epochs)[0]\n",
    "\n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-8)\n",
    "            # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5,\n",
    "            #                                                 max_lr=1e-2, epochs=epochs, steps_per_epoch=len(train_dataloader))\n",
    "            #torch.optim.Adam(model.parameters(),betas=(0.9, 0.99), lr=lr, weight_decay=0.00001,eps=1e-5)\n",
    "            if len(gpus) > 1:\n",
    "                model = nn.DataParallel(model, device_ids=gpus, output_device=gpus[0])\n",
    "\n",
    "\n",
    "            loss_tr = nn.BCELoss()\n",
    "            loss_tr1 = nn.BCELoss(reduction='none')\n",
    "            if obj_max == 1:\n",
    "                best_valid_metric = 0\n",
    "            else:\n",
    "                best_valid_metric = 1e9\n",
    "            not_improve_epochs = 0\n",
    "            if do_train:\n",
    "                for epoch in range(epochs):\n",
    "                    # if epoch <= 13:\n",
    "                    #     continue\n",
    "                    np.random.seed(666*epoch)\n",
    "                    train_loss = 0.0\n",
    "                    train_num = 0\n",
    "                    scheduler.step(model,epoch,epochs)\n",
    "                    model.train()\n",
    "                    bar = tqdm(train_dataloader)\n",
    "                    for data in bar:\n",
    "                        optimizer.zero_grad()\n",
    "                        for k in data:\n",
    "                            data[k] = data[k].cuda()\n",
    "                        y = data['batch_y']\n",
    "                        if use_amp:\n",
    "                            # torch.amp (automatic mixed precision) => mixed precision tries to match each op to its appropriate datatype.\n",
    "                            # autocast() => Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision. \n",
    "                            with amp.autocast():\n",
    "                                outputs = model(data)\n",
    "                                # loss_series = loss_tr1(series_outputs,y.repeat(1,13))\n",
    "                                # loss_series = (loss_series * data['batch_mask']).sum() / data['batch_mask'].sum()\n",
    "                                # if epoch < 30:\n",
    "                                #     loss = loss_series\n",
    "                                # else:\n",
    "                                loss = loss_tr(outputs,y) #+ loss_series # 0.5 * (loss_tr(outputs,y) + loss_feature(feature,y))\n",
    "                            if str(loss.item()) == 'nan': continue\n",
    "                            scaler.scale(loss).backward()\n",
    "                            torch.nn.utils.clip_grad_norm(model.parameters(), clipnorm)\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            outputs = model(data)\n",
    "                            loss = loss_tr(outputs,y)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        # scheduler.step()\n",
    "                        train_num += data['batch_feature'].shape[0]\n",
    "                        train_loss += data['batch_feature'].shape[0] * loss.item()\n",
    "                        bar.set_description('loss: %.4f' % (loss.item()))\n",
    "\n",
    "                    train_loss /= train_num\n",
    "\n",
    "                    # eval\n",
    "                    model.eval()\n",
    "                    valid_preds = []\n",
    "                    for data in tqdm(valid_dataloader):\n",
    "                        for k in data:\n",
    "                            data[k] = data[k].cuda()\n",
    "                        with torch.no_grad():\n",
    "                            if logit:\n",
    "                                outputs = model(data).sigmoid()\n",
    "                                # feature,outputs = model(data)\n",
    "                                # outputs = outputs.sigmoid()\n",
    "                            else:\n",
    "                                outputs = model(data)\n",
    "                                # feature,outputs = model(data)\n",
    "                        valid_preds.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "                    valid_preds = np.concatenate(valid_preds).reshape(-1)\n",
    "                    valid_Y = train_y.loc[val_index,label_name].values # oof train\n",
    "                    valid_mean = np.mean(valid_preds)\n",
    "                    valid_metric = Metric(valid_Y,valid_preds)\n",
    "\n",
    "                    if obj_max*(valid_metric) > obj_max*best_valid_metric:\n",
    "                        if len(gpus) > 1:\n",
    "                            torch.save(model.module.state_dict(),output_path + 'fold%s.ckpt'%fold)\n",
    "                        else:\n",
    "                            torch.save(model.state_dict(),output_path + 'fold%s.ckpt'%fold)\n",
    "                        not_improve_epochs = 0\n",
    "                        best_valid_metric = valid_metric\n",
    "                        Write_log(log,'[epoch %s] lr: %.6f, train_loss: %.6f, valid_metric: %.6f, valid_mean:%.6f'%(epoch,optimizer.param_groups[0]['lr'],train_loss,valid_metric,valid_mean))\n",
    "                    else:\n",
    "                        not_improve_epochs += 1\n",
    "                        Write_log(log,'[epoch %s] lr: %.6f, train_loss: %.6f, valid_metric: %.6f, valid_mean:%.6f, NIE +1 ---> %s'%(epoch,optimizer.param_groups[0]['lr'],train_loss,valid_metric,valid_mean,not_improve_epochs))\n",
    "                        if not_improve_epochs >= patience:\n",
    "                            break\n",
    "\n",
    "            state_dict = torch.load(output_path + 'fold%s.ckpt'%fold, torch.device('cuda' if torch.cuda.is_available() else 'cpu') )\n",
    "\n",
    "            model = model_class(223,(6372+13)*2,1,3,128,use_series_oof=use_series_oof)\n",
    "            model.cuda()\n",
    "            model.load_state_dict(state_dict)\n",
    "            if len(gpus) > 1:\n",
    "                model = nn.DataParallel(model, device_ids=gpus, output_device=gpus[0])\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            valid_preds = []\n",
    "            valid_Y = []\n",
    "            for data in tqdm(valid_dataloader):\n",
    "                for k in data:\n",
    "                    data[k] = data[k].cuda()\n",
    "                with torch.no_grad():\n",
    "                    if logit:\n",
    "                        outputs = model(data).sigmoid()\n",
    "                        # feature,outputs = model(data)\n",
    "                        # outputs = outputs.sigmoid()\n",
    "                    else:\n",
    "                        outputs = model(data)\n",
    "                        # feature,outputs = model(data)\n",
    "                valid_preds.append(outputs.detach().cpu().numpy())\n",
    "                valid_Y.append(y.detach().cpu().numpy())\n",
    "\n",
    "            valid_preds = np.concatenate(valid_preds).reshape(-1)\n",
    "            valid_Y = train_y.loc[val_index,label_name].values # oof train\n",
    "            valid_mean = np.mean(valid_preds)\n",
    "            valid_metric = Metric(valid_Y,valid_preds)\n",
    "            Write_log(log,'[fold %s] best_valid_metric: %.6f, best_valid_mean: %.6f'%(fold,valid_metric,valid_mean))\n",
    "\n",
    "            all_valid_metric.append(valid_metric)\n",
    "            oof.loc[val_index,label_name] = valid_preds\n",
    "            oof.loc[val_index,'fold'] = fold\n",
    "            train_folds.append(fold)\n",
    "\n",
    "        mean_valid_metric = np.mean(all_valid_metric)\n",
    "        Write_log(log,'all valid mean metric:%.6f'%(mean_valid_metric))\n",
    "        oof.loc[oof['fold'].isin(train_folds)].to_csv(output_path + 'oof.csv',index=False)\n",
    "\n",
    "        if test is None:\n",
    "            log.close()\n",
    "            os.rename(output_path + 'train.log', output_path + 'train_%.6f.log'%mean_valid_metric)\n",
    "\n",
    "        log_df = pd.DataFrame({'run_id':[run_id],'folds':folds,'metric':[round(mean_valid_metric,6)],'lb':[np.nan],'remark':[config['remark']]})\n",
    "        if not os.path.exists(output_root + 'experiment_log.csv'):\n",
    "            log_df.to_csv(output_root + 'experiment_log.csv',index=False)\n",
    "        else:\n",
    "            log_df.to_csv(output_root + 'experiment_log.csv',index=False,mode='a',header=None)\n",
    "\n",
    "    if test is not None:\n",
    "        if train is None:\n",
    "            log = open(output_path + 'test.log','w', buffering=1)\n",
    "            Write_log(log,str(config)+'\\n')\n",
    "        test_series,test_feature,test_series_idx = test\n",
    "\n",
    "        sub = test_feature[-len(test_series_idx):][[id_name]].reset_index(drop=True)\n",
    "        sub['prediction'] = 0\n",
    "\n",
    "        test_dataset = TaskDataset(test_series,test_feature,test_series_idx)\n",
    "        test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False, drop_last=False, collate_fn=test_dataset.collate_fn,num_workers=num_workers)\n",
    "        models = []\n",
    "        for fold in range(folds):\n",
    "            if not os.path.exists(output_path + 'fold%s.ckpt'%fold):\n",
    "                continue\n",
    "            model = model_class(223,(6372+13)*2,1,3,128,use_series_oof=use_series_oof)\n",
    "            model.cuda()\n",
    "            state_dict = torch.load(output_path + 'fold%s.ckpt'%fold, torch.device('cuda') )\n",
    "            model.load_state_dict(state_dict)\n",
    "            if len(gpus) > 1:\n",
    "                model = nn.DataParallel(model, device_ids=gpus, output_device=gpus[0])\n",
    "\n",
    "            model.eval()\n",
    "            models.append(model)\n",
    "        print('model count:',len(models))\n",
    "        test_preds = []\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_dataloader):\n",
    "                for k in data:\n",
    "                    data[k] = data[k].cuda()\n",
    "\n",
    "                if logit:\n",
    "                    # outputs = model(data).sigmoid()\n",
    "                    outputs = torch.stack([m(data).sigmoid() for m in models],0).mean(0)\n",
    "                    # feature,outputs = model(data)\n",
    "                    # outputs = outputs.sigmoid()\n",
    "                else:\n",
    "                    # outputs = model(data)\n",
    "                    outputs = torch.stack([m(data) for m in models],0).mean(0)\n",
    "                    # feature,outputs = model(data)\n",
    "                test_preds.append(outputs.cpu().detach().numpy())\n",
    "        test_preds = np.concatenate(test_preds).reshape(-1)\n",
    "        test_mean = np.mean(test_preds)\n",
    "        Write_log(log,'test_mean: %.6f'%(test_mean))\n",
    "        sub['prediction'] = test_preds\n",
    "        sub.to_csv(output_path+'submission.csv.zip',index=False, compression='zip')\n",
    "    else:\n",
    "        sub = None\n",
    "\n",
    "#     if save_dir in output_path:\n",
    "#         os.rename(output_path,output_root+run_id+'/')\n",
    "\n",
    "#     return oof,sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b4f4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_parquet('nn_series_train_feature')\n",
    "df['idx'] = df.index\n",
    "\n",
    "# getting 'index range' of each customer_ID.\n",
    "series_idx = df.groupby('customer_ID',sort=False)['idx'].agg(['min','max'])\n",
    "    # series_idx => \n",
    "    #                                                                 min  max\n",
    "    # customer_ID\n",
    "    # 0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a 0    12\n",
    "    # 00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5 13   25\n",
    "    # ...\n",
    "    \n",
    "series_idx['feature_idx'] = np.arange(len(series_idx))\n",
    "    # len(series_idx) => 1383534\n",
    "    # np.arange(len(series_idx)) => array([      0,       1,       2, ..., 1383531, 1383532, 1383533])\n",
    "    \n",
    "df = df.drop(['idx'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f675dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_parquet(f'train_labels')\n",
    "f = pd.read_parquet(f'nn_manual_train_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154d8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb36470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61974314",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_config = {\n",
    "    'id_name':id_name,\n",
    "    'feature_name':[],\n",
    "    'label_name':label_name,\n",
    "    'obj_max': 1,\n",
    "    'epochs': 9, # 10\n",
    "    'smoothing': 0.001,\n",
    "    'clipnorm': 1,\n",
    "    'patience': 100,\n",
    "    'lr': 123e-6, # 3e-4\n",
    "    'batch_size': 256, # 256\n",
    "    'folds': 5,\n",
    "    'seed': seed,\n",
    "    'remark': remark\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of dataframe 'series_idx' to array.\n",
    "# series_idx.values => \n",
    "# array([[       0,       12,        0],\n",
    "#        [      13,       25,        1],\n",
    "#        [      26,       38,        2],\n",
    "test = None\n",
    "NN_train_and_predict([df,f,y,series_idx.values],test,Amodel,nn_config,use_series_oof=True,output_root='./output/',run_id='NN_with_series_and_manual_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b98528fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2306: 100%|█████████████████████████| 1434/1434 [25:56<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.2509: 100%|█████████████████████████| 1434/1434 [25:52<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.08s/it]\n",
      "loss: 0.2070: 100%|█████████████████████████| 1434/1434 [25:57<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.1889: 100%|█████████████████████████| 1434/1434 [25:51<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:34<00:00,  1.10s/it]\n",
      "loss: 0.2450: 100%|█████████████████████████| 1434/1434 [26:02<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.2210: 100%|█████████████████████████| 1434/1434 [25:52<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:32<00:00,  1.09s/it]\n",
      "loss: 0.2273: 100%|█████████████████████████| 1434/1434 [25:56<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:32<00:00,  1.09s/it]\n",
      "loss: 0.2048: 100%|█████████████████████████| 1434/1434 [25:51<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.1890: 100%|█████████████████████████| 1434/1434 [25:53<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:36<00:00,  1.10s/it]\n",
      "loss: 0.2179: 100%|█████████████████████████| 1434/1434 [25:50<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.08s/it]\n",
      "loss: 0.2169: 100%|█████████████████████████| 1434/1434 [25:56<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.09s/it]\n",
      "loss: 0.2278: 100%|█████████████████████████| 1434/1434 [25:51<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.2234: 100%|█████████████████████████| 1434/1434 [25:56<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:32<00:00,  1.09s/it]\n",
      "loss: 0.1930: 100%|█████████████████████████| 1434/1434 [25:55<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:33<00:00,  1.09s/it]\n",
      "loss: 0.3093: 100%|█████████████████████████| 1434/1434 [25:59<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.1787: 100%|█████████████████████████| 1434/1434 [25:58<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.2571: 100%|█████████████████████████| 1434/1434 [25:54<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.1944: 100%|█████████████████████████| 1434/1434 [25:56<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.2381: 100%|█████████████████████████| 1434/1434 [25:54<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.08s/it]\n",
      "loss: 0.1746: 100%|█████████████████████████| 1434/1434 [25:48<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:27<00:00,  1.08s/it]\n",
      "loss: 0.2278: 100%|█████████████████████████| 1434/1434 [25:44<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.08s/it]\n",
      "loss: 0.2931: 100%|█████████████████████████| 1434/1434 [25:42<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.2050: 100%|█████████████████████████| 1434/1434 [25:44<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:28<00:00,  1.08s/it]\n",
      "loss: 0.2130: 100%|█████████████████████████| 1434/1434 [25:50<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.08s/it]\n",
      "loss: 0.2598: 100%|█████████████████████████| 1434/1434 [25:49<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.2467: 100%|█████████████████████████| 1434/1434 [25:47<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:28<00:00,  1.08s/it]\n",
      "loss: 0.1918: 100%|█████████████████████████| 1434/1434 [26:20<00:00,  1.10s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:35<00:00,  1.10s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:29<00:00,  1.08s/it]\n",
      "loss: 0.1935: 100%|█████████████████████████| 1434/1434 [26:23<00:00,  1.10s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.2865: 100%|█████████████████████████| 1434/1434 [26:28<00:00,  1.11s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:41<00:00,  1.12s/it]\n",
      "loss: 0.1880: 100%|█████████████████████████| 1434/1434 [26:00<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:32<00:00,  1.09s/it]\n",
      "loss: 0.2837: 100%|█████████████████████████| 1434/1434 [26:04<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:43<00:00,  1.12s/it]\n",
      "loss: 0.2553: 100%|█████████████████████████| 1434/1434 [25:57<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:30<00:00,  1.09s/it]\n",
      "loss: 0.2503: 100%|█████████████████████████| 1434/1434 [26:08<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:36<00:00,  1.10s/it]\n",
      "loss: 0.1852: 100%|█████████████████████████| 1434/1434 [25:54<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:33<00:00,  1.10s/it]\n",
      "loss: 0.1511: 100%|█████████████████████████| 1434/1434 [26:00<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:32<00:00,  1.09s/it]\n",
      "loss: 0.2317: 100%|█████████████████████████| 1434/1434 [26:07<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:41<00:00,  1.12s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.1892: 100%|█████████████████████████| 1434/1434 [26:08<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.1904: 100%|█████████████████████████| 1434/1434 [26:29<00:00,  1.11s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:43<00:00,  1.13s/it]\n",
      "loss: 0.2594: 100%|█████████████████████████| 1434/1434 [25:54<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:46<00:00,  1.13s/it]\n",
      "loss: 0.2365: 100%|█████████████████████████| 1434/1434 [25:50<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.2555: 100%|█████████████████████████| 1434/1434 [26:02<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:33<00:00,  1.10s/it]\n",
      "loss: 0.2298: 100%|█████████████████████████| 1434/1434 [25:53<00:00,  1.08s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:33<00:00,  1.10s/it]\n",
      "loss: 0.2096: 100%|█████████████████████████| 1434/1434 [25:57<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:28<00:00,  1.08s/it]\n",
      "loss: 0.2148: 100%|█████████████████████████| 1434/1434 [25:57<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:31<00:00,  1.09s/it]\n",
      "loss: 0.2325: 100%|█████████████████████████| 1434/1434 [26:09<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:32<00:00,  1.09s/it]\n",
      "100%|█████████████████████████████████████████| 359/359 [06:36<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "test=None\n",
    "NN_train_and_predict([df,f,y,series_idx.values],test,Amodel,nn_config,use_series_oof=False,output_root='./output/',run_id='NN_with_series_feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebec6d5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac000e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_parquet('nn_series_test_feature')\n",
    "df['idx'] = df.index\n",
    "\n",
    "# getting 'index range' of each customer_ID.\n",
    "series_idx = df.groupby('customer_ID',sort=False)['idx'].agg(['min','max'])\n",
    "    # series_idx => \n",
    "    #                                                                 min  max\n",
    "    # customer_ID\n",
    "    # 0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a 0    12\n",
    "    # 00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5 13   25\n",
    "    # ...\n",
    "    \n",
    "series_idx['feature_idx'] = np.arange(len(series_idx))\n",
    "    # len(series_idx) => 1383534\n",
    "    # np.arange(len(series_idx)) => array([      0,       1,       2, ..., 1383531, 1383532, 1383533])\n",
    "    \n",
    "df = df.drop(['idx'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d51025",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_parquet(f'nn_manual_test_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f8caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model count: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3612/3612 [1:04:53<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "train = None\n",
    "NN_train_and_predict(train,[df,f,series_idx.values],Amodel,nn_config,use_series_oof=True,output_root='./output/',run_id='NN_with_series_and_manual_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0bd8f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model count: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 3612/3612 [1:05:39<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "train = None\n",
    "NN_train_and_predict(train,[df,f,series_idx.values],Amodel,nn_config,use_series_oof=False,output_root='./output/',run_id='NN_with_series_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25955bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
