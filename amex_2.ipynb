{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817e952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a54909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_column', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_seq_items', None)\n",
    "# pd.set_option('display.max_colwidth', None) # 500\n",
    "# pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace39ee2",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18e20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c733f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8503032f",
   "metadata": {},
   "source": [
    "## DataFrames size reduction and other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617f052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65\n",
    "def reduce_mem_usage(props):\n",
    "#     start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "#     print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "#     NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "            \n",
    "#             # Print current column type\n",
    "#             print(\"******************************\")\n",
    "#             print(\"Column: \",col)\n",
    "#             print(\"dtype before: \",props[col].dtype)\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "#             # Integer does not support NA, therefore, NA needs to be filled\n",
    "#             if not np.isfinite(props[col]).all(): \n",
    "#                 NAlist.append(col)\n",
    "#                 props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column dtype is int.\n",
    "            if 'int' in props[col].dtype.name:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "#             # Print new column type\n",
    "#             print(\"dtype after: \",props[col].dtype)\n",
    "#             print(\"******************************\")\n",
    "    \n",
    "#     # Print final result\n",
    "#     print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "#     mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "#     print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "#     print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "#     return props, NAlist\n",
    "    return props\n",
    "\n",
    "def pad_target(x):\n",
    "    # pad target (whose length is less than 13) with np.nan values at start.\n",
    "    \n",
    "    t = np.zeros(13)\n",
    "    t[:-len(x)] = 0. # np.nan\n",
    "    t[-len(x):] = x\n",
    "    return list(t)\n",
    "\n",
    "# limiting 'count of distinct values' to be less than or equal to 255 (max_bin), by removing infrequent values.\n",
    "def GreedyFindBin(distinct_values, counts, num_distinct_values, max_bin, total_cnt, min_data_in_bin=3):\n",
    "    # distinct_values = vc.index.values\n",
    "    # counts = vc.values\n",
    "    # num_distinct_values = len(vc) e.g., 284\n",
    "    # max_bin = 255\n",
    "    # total_cnt = vc.sum() e.g., 1383534\n",
    "\n",
    "    # distinct_values => \n",
    "    # [-8900. -8100. -8000. -7900. -7800. -7700. -7600. -7500. -7400. -7300.\n",
    "    #  -7200. -7100. -7000. -6900. -6800. -6700. -6600. -6500. -6400. -6300.\n",
    "    #     ..\n",
    "    #  18799. 18899. 18999. 19099. 19199. 19299. 19399. 19499. 19599. 19699.\n",
    "    #  19799. 19899. 19999. 20099.]    \n",
    "    \n",
    "    # counts => \n",
    "    # [    1     1     1     1     2     2     4     1     2     3     5     7\n",
    "    #      4     5    11     4     9    13    13    11    20    10    30    21\n",
    "    #     ..\n",
    "    #  14966 11184 13507 10751 13033 10588 12653  9195 10650  8760 10421  8375\n",
    "    #   9463  6798  8235  6411  8216  6324 27516     1]    \n",
    "\n",
    "    bin_upper_bound=list();\n",
    "    assert(max_bin>0)\n",
    "\n",
    "    # The number of feature values is less than the number of max_bin, directly take the midpoint of distinct_values and place it.\n",
    "    if num_distinct_values <= max_bin:\n",
    "        cur_cnt_inbin = 0\n",
    "        for i in range(num_distinct_values-1):\n",
    "            cur_cnt_inbin += counts[i]\n",
    "\n",
    "            if cur_cnt_inbin >= min_data_in_bin:\n",
    "                bin_upper_bound.append((distinct_values[i] + distinct_values[i + 1]) / 2.0)\n",
    "                cur_cnt_inbin = 0\n",
    "\n",
    "        cur_cnt_inbin += counts[num_distinct_values - 1];\n",
    "#         bin_upper_bound.append(float('Inf'))\n",
    "\n",
    "    else:\n",
    "        if min_data_in_bin>0:\n",
    "            max_bin=min(max_bin,total_cnt//min_data_in_bin)\n",
    "                # max_bin => 255\n",
    "            max_bin=max(max_bin,1)\n",
    "                # max_bin => 255            \n",
    "\n",
    "        mean_bin_size=total_cnt/max_bin\n",
    "            # mean_bin_size => 5425.623529411765\n",
    "        rest_bin_cnt = max_bin\n",
    "        rest_sample_cnt = total_cnt\n",
    "            # rest_sample_cnt, rest_bin_cnt => 1383534, 255\n",
    "\n",
    "        is_big_count_value=[False]*num_distinct_values        \n",
    "            # is_big_count_value => [False, False,  ..., False]\n",
    "            # 284 elements.\n",
    "\n",
    "        for i in range(num_distinct_values):\n",
    "            if counts[i] >= mean_bin_size:\n",
    "                is_big_count_value[i] = True\n",
    "                rest_bin_cnt-=1\n",
    "                rest_sample_cnt -= counts[i]\n",
    "\n",
    "            # rest_sample_cnt, rest_bin_cnt => 207531 121\n",
    "        mean_bin_size = rest_sample_cnt/rest_bin_cnt\n",
    "            # mean_bin_size => 1715.1322314049587\n",
    "\n",
    "        upper_bounds=[float('Inf')]*max_bin\n",
    "        lower_bounds=[float('Inf')]*max_bin\n",
    "        bin_cnt = 0\n",
    "        lower_bounds[bin_cnt] = distinct_values[0]\n",
    "        cur_cnt_inbin = 0\n",
    "        \n",
    "        for i in range(num_distinct_values-1):\n",
    "                # num_distinct_values => 284\n",
    "            if not is_big_count_value[i]:\n",
    "                rest_sample_cnt -= counts[i]\n",
    "            \n",
    "                # rest_sample_cnt, rest_bin_cnt => 207530, 121\n",
    "            cur_cnt_inbin += counts[i]            \n",
    "\n",
    "            # If the cur_cnt_inbin is too small, accumulate the next value until the condition is met and enter the loop.\n",
    "            # Need a new bin if the current feature values need to be separated into a bin, or the current count of several feature values exceeds mean_bin_size, or the next one needs to be independently bucketed.\n",
    "            if is_big_count_value[i] or cur_cnt_inbin >= mean_bin_size or \\\n",
    "            (is_big_count_value[i + 1] and cur_cnt_inbin >= max(1.0, mean_bin_size * 0.5)):\n",
    "                # counts[i] => 205\n",
    "                # cur_cnt_inbin, distinct_values[i] => 1886, -4000.0\n",
    "                # bin_cnt => 0\n",
    "                upper_bounds[bin_cnt] = distinct_values[i] # The largest value of the i-th bin is distinct_values[i].\n",
    "                bin_cnt+=1\n",
    "                # distinct_values[i + 1] => -3900.0\n",
    "                lower_bounds[bin_cnt] = distinct_values[i + 1] # The minimum of the next bin is distinct_values[i + 1], pay attention to ++bin first.\n",
    "                if bin_cnt >= max_bin - 1:\n",
    "                    break\n",
    "                cur_cnt_inbin = 0\n",
    "                if not is_big_count_value[i]:\n",
    "                    rest_bin_cnt-=1\n",
    "                    mean_bin_size = rest_sample_cnt / rest_bin_cnt\n",
    "\n",
    "        # update bin upper bound (Similar to the operation where the number of feature values is less than the number of max_bins, take the mean of the current value and the next value as the dividing point of the bucket).\n",
    "        for i in range(bin_cnt-1):\n",
    "            bin_upper_bound.append((upper_bounds[i] + lower_bounds[i + 1]) / 2.0)\n",
    "#         bin_upper_bound.append(float('Inf'))\n",
    "\n",
    "        # bin_upper_bound =>\n",
    "        # [-3950.0, -3250.0, -2750.0, -2350.0, -2050.0, -1750.0, -1450.0, -1150.0, -950.0, -750.0,\n",
    "        #  -550.0, -350.0, -150.0, -50.0, 49.5, 249.0, 449.0, 649.0, 849.0, 1049.0,\n",
    "        #  ... \n",
    "        #  18049.0, 18149.0, 18249.0, 18349.0, 18449.0, 18549.0, 18649.0, 18749.0, 18849.0, 18949.0,\n",
    "        #  19049.0, 19149.0, 19249.0, 19349.0, 19449.0, 19549.0, 19649.0, 19749.0, 19849.0, 19949.0]\n",
    "        # len(bin_upper_bound) => 203\n",
    "    return bin_upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86f83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in ['cat','num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num']:\n",
    "    df = reduce_mem_usage(pd.read_parquet(f'{root}/{fn}_feature'))\n",
    "\n",
    "    if 'last' in fn:\n",
    "        # appending 'lastX_' to columns of dataframes 'last3_cat','last3_num','last3_diff' and 'last6_num'.\n",
    "        pre = '_'.join(fn.split('_')[:-1])+'_'\n",
    "        df = df.rename({col:pre+col for col in df.columns if col != 'customer_ID'}, axis=1)\n",
    "        \n",
    "    df.to_parquet(f'{root}/{fn}_feature', compression='gzip', index=False)\n",
    "del df\n",
    "_ = gc.collect()\n",
    "# also, copy ['cat','num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num'] files to the inside of folder 'extra'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c759350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc6ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f054f2",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a65273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1a4d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 208/208 [00:10<00:00, 19.11it/s]\n",
      " 22%|████████▋                               | 232/1063 [03:18<10:38,  1.30it/s]/home/na/miniconda3/envs/base_2/lib/python3.7/site-packages/ipykernel_launcher.py:169: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "100%|███████████████████████████████████████| 1063/1063 [08:45<00:00,  2.02it/s]\n",
      "100%|███████████████████████████████████████| 1063/1063 [08:38<00:00,  2.05it/s]\n",
      "100%|█████████████████████████████████████████| 178/178 [00:14<00:00, 12.59it/s]\n",
      "100%|█████████████████████████████████████████| 148/148 [00:04<00:00, 32.16it/s]\n",
      "100%|█████████████████████████████████████████| 886/886 [06:00<00:00,  2.46it/s]\n",
      "100%|█████████████████████████████████████████| 886/886 [05:58<00:00,  2.47it/s]\n",
      "100%|█████████████████████████████████████████| 886/886 [06:12<00:00,  2.38it/s]\n",
      "100%|███████████████████████████████████████| 1063/1063 [08:45<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for fn in ['cat','num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num']:\n",
    "    df = pd.read_parquet(f'{root}/{fn}_feature')    \n",
    "    for col in tqdm(df.columns):\n",
    "        if col not in ['customer_ID','S_2']:\n",
    "            vc = df[col].value_counts().sort_index()\n",
    "                # (for column 'last3_P_2_sum') vc =>         \n",
    "                #             ...\n",
    "                # -7900.0         1\n",
    "                # -7800.0         2\n",
    "                #             ...  \n",
    "                #  19699.0     6411\n",
    "                #  19799.0     8216 \n",
    "                # Name: last3_P_2_sum, Length: 284, dtype: int64\n",
    "\n",
    "                # len(vc), vc.sum() => 284, 1383534\n",
    "            bins = GreedyFindBin(vc.index.values,vc.values,len(vc),255,vc.sum())    \n",
    "                # bins => [-3950.0, -3250.0, -2750.0, -2350.0, .., 19549.0, 19649.0, 19749.0, 19849.0, 19949.0]  \n",
    "                # len(bins) => 204\n",
    "\n",
    "                # len(df[col]) => 1383534\n",
    "\n",
    "            if not -np.inf in bins: bins = [-np.inf]+bins            \n",
    "            if not np.inf in bins: bins = bins+[np.inf]\n",
    "\n",
    "            df[col] = np.digitize(df[col], bins)\n",
    "                # numpy.digitize(x, bins, right=False) => Return the indices of the bins to which each value in input array belongs.\n",
    "                # df[col] => \n",
    "                # 0           78\n",
    "                # 1          163\n",
    "                #           ... \n",
    "                # 1383532    115\n",
    "                # 1383533    172    \n",
    "\n",
    "                # len(df[col]) => 1383534            \n",
    "\n",
    "            df.loc[df[col]==len(bins),col] = 0 # put zeros for values matching the 'len(bins)'.\n",
    "\n",
    "            df[col] = df[col] / df[col].max() # normalize\n",
    "                # df[col] => \n",
    "                # 0          0.382353\n",
    "                # 1          0.799020\n",
    "                #              ...   \n",
    "                # 1383532    0.563725\n",
    "                # 1383533    0.843137   \n",
    "    df = reduce_mem_usage(df)\n",
    "    df.to_parquet(f'{root}/{fn}_feature', compression='gzip', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615d248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d7ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab22350",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.read_csv('./o_debug/LGB_with_series_feature/oof.csv') # train data\n",
    "sub = pd.read_csv('./o_debug/LGB_with_series_feature/submission.csv.zip') # test data\n",
    "oof = reduce_mem_usage(oof)\n",
    "sub = reduce_mem_usage(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bead1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = oof.groupby('customer_ID',sort=False)['target'].agg(lambda x:pad_target(x))\n",
    "sub = sub.groupby('customer_ID',sort=False)['prediction'].agg(lambda x:pad_target(x))\n",
    "tmp = pd.concat([oof,sub])\n",
    "    # tmp =>\n",
    "    # [0.0017161552068444, 0.00154039329695, 0.001630075701532, ..., 0.0007328170974785, 0.0009232219338176]\n",
    "    # 13 elements\n",
    "del oof, sub\n",
    "_ = gc.collect()\n",
    "tmp = pd.DataFrame(data=tmp.tolist(), index=tmp.index, columns=['target%s'%i for i in range(1,14)])\n",
    "tmp['customer_ID'] = tmp.index\n",
    "tmp = reduce_mem_usage(tmp)\n",
    "tmp.to_parquet(f'tmp_feature', compression='gzip', index=False)\n",
    "# also, copy tmp file to the inside of folder 'extra'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621af47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "537d300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc469c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading manual features (pandas library).\n",
    "# dfs = []\n",
    "# for fn in ['cat','num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num']:\n",
    "#     if len(dfs) == 0:\n",
    "#         dfs.append(pd.read_parquet(f'{root}/{fn}_feature'))\n",
    "#     else:\n",
    "#         dfs.append(pd.read_parquet(f'{root}/{fn}_feature')).drop([id_name],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598fa5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading manual features (polars library).\n",
    "df = pl.read_parquet(f'cat_feature')\n",
    "for fn in ['num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num','tmp']:\n",
    "    df = df.join(pl.read_parquet(f'{root}/{fn}_feature'), on=\"customer_ID\", how=\"left\", ) \n",
    "    \n",
    "df.write_parquet(f'{root}/nn_all_feature', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912023e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
